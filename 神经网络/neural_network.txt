class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), activation=’relu’, solver=’adam’, alpha=0.0001, 
batch_size=’auto’, learning_rate=’constant’, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True,
 random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, 
 early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)[source]




参数

1）hidden_​​layer_sizes
tuple，length = n_layers - 2，默认值（100，）第i个元素表示第i个隐藏层中的神经元数量。

2）激活
{‘identity’，‘logistic’，‘tanh’，‘relu’}，默认’relu’       隐藏层的激活函数：‘identity’，无操作激活，对实现线性瓶颈很有用，返回f（x）= x；‘logistic’，logistic sigmoid函数，返回f（x）= 1 /（1 + exp（-x））；‘tanh’，双曲tan函数，返回f（x）= tanh（x）；‘relu’，整流后的线性单位函数，返回f（x）= max（0，x）

3）slover
{‘lbfgs’，‘sgd’，‘adam’}，默认’adam’。权重优化的求解器：'lbfgs’是准牛顿方法族的优化器；'sgd’指的是随机梯度下降。'adam’是指由Kingma，Diederik和Jimmy Ba提出的基于随机梯度的优化器。注意：默认解算器“adam”在相对较大的数据集（包含数千个训练样本或更多）方面在训练时间和验证分数方面都能很好地工作。但是，对于小型数据集，“lbfgs”可以更快地收敛并且表现更好。

4）alpha
float，可选，默认为0.0001。L2惩罚（正则化项）参数。

5）batch_size
int，optional，默认’auto’。用于随机优化器的minibatch的大小。如果slover是’lbfgs’，则分类器将不使用minibatch。设置为“auto”时，batch_size = min（200，n_samples）

6）learning_rate
{‘常数’，‘invscaling’，‘自适应’}，默认’常数"。 用于权重更新。仅在solver ='sgd’时使用。'constant’是’learning_rate_init’给出的恒定学习率；'invscaling’使用’power_t’的逆缩放指数在每个时间步’t’逐渐降低学习速率learning_rate_， effective_learning_rate = learning_rate_init / pow（t，power_t）；只要训练损失不断减少，“adaptive”将学习速率保持为“learning_rate_init”。每当两个连续的时期未能将训练损失减少至少tol，或者如果’early_stopping’开启则未能将验证分数增加至少tol，则将当前学习速率除以5。

7）learning_rate_init
double，可选，默认为0.001。使用初始学习率。它控制更新权重的步长。仅在solver ='sgd’或’adam’时使用。

8）power_t
double，可选，默认为0.5。反缩放学习率的指数。当learning_rate设置为“invscaling”时，它用于更新有效学习率。仅在solver ='sgd’时使用。

9）max_iter
int，optional，默认值200。最大迭代次数。solver迭代直到收敛（由’tol’确定）或这个迭代次数。对于随机解算器（‘sgd’，‘adam’），请注意，这决定了时期的数量（每个数据点的使用次数），而不是梯度步数。

10）shuffle
bool，可选，默认为True。仅在solver ='sgd’或’adam’时使用。是否在每次迭代中对样本进行洗牌。

11）random_state
int，RandomState实例或None，可选，默认无随机数生成器的状态或种子。如果是int，则random_state是随机数生成器使用的种子;如果是RandomState实例，则random_state是随机数生成器;如果为None，则随机数生成器是np.random使用的RandomState实例。

12）tol
float，optional，默认1e-4 优化的容忍度，容差优化。当n_iter_no_change连续迭代的损失或分数没有提高至少tol时，除非将learning_rate设置为’adaptive’，否则认为会达到收敛并且训练停止。

13）verbose
bool，可选，默认为False 是否将进度消息打印到stdout。

14）warm_start
bool，可选，默认为False，设置为True时，重用上一次调用的解决方案以适合初始化，否则，只需擦除以前的解决方案。请参阅词汇表。

15）momentum
float，默认0.9，梯度下降更新的动量。应该在0和1之间。仅在solver ='sgd’时使用。

16）nesterovs_momentum
布尔值，默认为True。是否使用Nesterov的势头。仅在solver ='sgd’和momentum> 0时使用。

17）early_stopping
bool，默认为False。当验证评分没有改善时，是否使用提前停止来终止培训。如果设置为true，它将自动留出10％的训练数据作为验证，并在验证得分没有改善至少为n_iter_no_change连续时期的tol时终止训练。仅在solver ='sgd’或’adam’时有效

18）validation_fraction
float，optional，默认值为0.1。将训练数据的比例留作早期停止的验证集。必须介于0和1之间。仅在early_stopping为True时使用

19）beta_1
float，optional，默认值为0.9，估计一阶矩向量的指数衰减率应为[0,1)。仅在solver ='adam’时使用

20）beta_2
float，可选，默认为0.999,估计一阶矩向量的指数衰减率应为[0,1)。仅在solver ='adam’时使用

21）epsilon
float，optional，默认值1e-8, adam稳定性的价值。 仅在solver ='adam’时使用

22）n_iter_no_change
int，optional，默认值10,不符合改进的最大历元数。 仅在solver ='sgd’或’adam’时有效






属性
备注




classes_
array or list of array of shape （n_classes，）每个输出的类标签。


loss_
float,使用损失函数计算的当前损失。


coefs_
list，length n_layers - 1,列表中的第i个元素表示对应于层i的权重矩阵。


intercepts_
list，length n_layers - 1,列表中的第i个元素表示对应于层i + 1的偏置矢量。


n_iter_
int，迭代次数。


n_layers_
int,层数。


n_outputs_
int,输出的个数。


out_activation_
string，输出激活函数的名称。






方法
备注




fit（X，y）
使模型适合数据矩阵X和目标y。


get_params（[deep]）
获取此估算器的参数。


predict（X）
使用多层感知器分类器进行预测


predict_log_proba（X）
返回概率估计的对数。


predict_proba（X）
概率估计。


score（X，y [，sample_weight]）
返回给定测试数据和标签的平均准确度。


set_params（** params）
设置此估算器的参数。
